{
  
    
        "post0": {
            "title": "Time Series Analysis of the US Treasury 10- Year Yield",
            "content": ". AR(p) Models . A time series model for the observed data $ {x_t }$ is a specification of the joint distribution (or possibly only the means and covariances) of a sequence of random variables $ {X_t }$ of which $ {x_t }$ is postulated to be a realization. . The causal autoregressive $AR(p)$ process is defined by $$ X_t- phi_1 X_{t-1} - ...- phi_p X_{t-p}=c + Z_t, {Z_t} sim WN(0, sigma^2). $$ . A time series $ {X_t }$ is (covariance) stationnary if the mean function $ mu_X(t):= E(X_t)$ is independent of $t$, and the autocovariance function (ACVF) of $ {X_t }$ at lag $h$ . $$ gamma_X(t+h,t) := Cov(X_{t+h},X_t) = mathbb{E}[(X_{t+h}- mu_X(t+h))(X_{t}- mu_X(t))] $$is independent of $t$ for each $h$. . To assess the degree of dependence in the data and to select a model for the data that reflects this, one of the important tools we use is the sample autocorrelation function (sample ACF) of the data. Let $ {X_t }$ be a stationary time series. The autocorrelation function of $ {X_t }$ at lag $h$ is . $$ rho_X(h):= frac{ gamma_X(h)}{ gamma_X(0)} = Cor(X_{t+h},X_{t}). $$Let $x_1,...,x_n$ be observations of a time series. The sample autocorrelation function is . $$ hat rho(h) = frac{ hat gamma(h)}{ hat gamma(0)}, $$where $ hat gamma(h)$ is the sample autocovariance function i.e., $ hat gamma(h): = n^{-1} sum_{t=1}^{n-|h|}(x_{n-|h|}- bar{x})(x_t- bar{x})$, for $ -n&lt;h&lt;n$ and $ bar{x}:=n^{-1} sum^n_{t=1} x_t$. . We define sample PACF in an analogous way. If we believe that the data are realized values of a stationary time series $ {X_t }$, then the sample ACF will provide us with an estimate of the ACF of $ {X_t }$. This estimate may suggest which of the many possible stationary time series models is a suitable candidate for representing the dependence in the data. For example, a sample ACF that is close to zero for all nonzero lags suggests that an appropriate model for the data might be iid noise. . A partial autocorrelation function (PACF) of an ARMA process $ {X_t }$ is the function $ alpha( cdot)$ defined by the equations . $$ alpha(0) = 1 text{and} alpha(h) = phi_{hh}, h geq 1, $$where $ phi_{hh}$ is the last component of $ Phi_h = Gamma^{-1}_{h} gamma_h$, $ Gamma_h$ is $h$-dimensional the covariance matrix and $ gamma_h = [ gamma(1), gamma(2),..., gamma(h)]&#39;$. . The partial autocorrelation function is a tool that exploits the fact that, whereas an $AR(p)$ process has an autocorrelation function that is infinite in extent, the partial autocorrelations are zero beyond lag $p$. We define sample PACF for observed data in an analogous way. . Reference . Peter J. Brockwell, Richard A. Davis, Introduction to time series and forecasting, third edition | . Load python libraries and Federal Reserve data . The following commands re-load the data and evaluates the presence and nature of missing values. . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . %cd /content/drive/MyDrive/&quot;Time Series Analysis&quot; . /content/drive/MyDrive/Time Series Analysis . %matplotlib inline import warnings import numpy as np import pandas as pd import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.tsa.ar_model import AutoReg from pylab import mpl, plt import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots plt.style.use(&#39;seaborn&#39;) mpl.rcParams[&#39;font.family&#39;] = &#39;serif&#39; . fred_data = pd.read_csv(&quot;fred_data.csv&quot;, index_col=&quot;DATE&quot;) . fred_data.head() . DGS3MO DGS1 DGS5 DGS10 DAAA DBAA DCOILWTICO . DATE . 2011-10-17 0.04 | 0.12 | 1.08 | 2.18 | 4.00 | 5.44 | 86.38 | . 2011-10-18 0.04 | 0.12 | 1.07 | 2.19 | 3.96 | 5.42 | 88.34 | . 2011-10-19 0.03 | 0.11 | 1.05 | 2.18 | 3.97 | 5.39 | 86.11 | . 2011-10-20 0.03 | 0.12 | 1.07 | 2.20 | 3.98 | 5.40 | 86.07 | . 2011-10-21 0.02 | 0.12 | 1.08 | 2.23 | 3.99 | 5.41 | 87.19 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . fig = px.line(fred_data, y=&#39;DGS10&#39;) fig.show(); . . . # Print out the counts of missing values fred_data.isna().sum() . DGS3MO 108 DGS1 108 DGS5 108 DGS10 108 DAAA 108 DBAA 108 DCOILWTICO 97 dtype: int64 . fred_data[fred_data.isna()[&quot;DGS10&quot;]==True].index . Index([&#39;2011-11-11&#39;, &#39;2011-11-24&#39;, &#39;2011-12-26&#39;, &#39;2012-01-02&#39;, &#39;2012-01-16&#39;, &#39;2012-02-20&#39;, &#39;2012-05-28&#39;, &#39;2012-07-04&#39;, &#39;2012-09-03&#39;, &#39;2012-10-08&#39;, ... &#39;2020-11-11&#39;, &#39;2020-11-26&#39;, &#39;2020-12-25&#39;, &#39;2021-01-01&#39;, &#39;2021-01-18&#39;, &#39;2021-02-15&#39;, &#39;2021-05-31&#39;, &#39;2021-07-05&#39;, &#39;2021-09-06&#39;, &#39;2021-10-11&#39;], dtype=&#39;object&#39;, name=&#39;DATE&#39;, length=108) . # in the bond market of the US. # Define fred_data0 as sub matrix with nonmissing data for DGS10 fred_data0 = fred_data[fred_data.isna()[&quot;DGS10&quot;]==False] . fred_data0.isna().sum() . DGS3MO 0 DGS1 0 DGS5 0 DGS10 0 DAAA 2 DBAA 2 DCOILWTICO 9 dtype: int64 . DGS10_daily = fred_data0[[&quot;DGS10&quot;]] . len(DGS10_daily) . 2502 . DGS10_daily.head() . DGS10 . DATE . 2011-10-17 2.18 | . 2011-10-18 2.19 | . 2011-10-19 2.18 | . 2011-10-20 2.20 | . 2011-10-21 2.23 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Create weekly and monthly time series . # to an Open/High/Low/Close series on a periodicity lower than the input data object. # Plot OHLC chart which shows the open, high, low, and close price for a given period. warnings.filterwarnings(&quot;ignore&quot;) DGS10_daily[&#39;Date&#39;] = pd.to_datetime(DGS10_daily.index, format=&#39;%Y/%m/%d&#39;) DGS10_daily = DGS10_daily.set_index([&#39;Date&#39;]) DGS10_daily.columns = [&#39;DGS10_daily&#39;] def resample_plot(data, how): df = pd.DataFrame(columns=[&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;]) ohlc = {&#39;open&#39;: lambda x: x[0], &#39;high&#39;: max, &#39;low&#39;: min, &#39;close&#39;: lambda x: x[-1]} for key in ohlc.keys(): df[key] = data.resample(how).apply(ohlc[key]) fig = go.Figure(data=[go.Candlestick(x=df.index, open=df.loc[:,&#39;open&#39;], high=df.loc[:,&#39;high&#39;], low=df.loc[:,&#39;low&#39;], close=df.loc[:,&#39;close&#39;]) ]) f = lambda x: &#39;week&#39; if x==&#39;W&#39; else &#39;month&#39; fig.update_layout(title=&#39;OHLC chart for {}&#39;.format(f(how)), yaxis_title=&#39;DGS10&#39;, xaxis_rangeslider_visible=False) fig.show() return df . OHLC_weekly = resample_plot(DGS10_daily,&#39;W&#39;) . . . OHLC_monthly = resample_plot(DGS10_daily,&#39;M&#39;) . . . DGS10_weekly = OHLC_weekly[[&#39;close&#39;]] DGS10_weekly.columns = [&#39;DGS10_weekly&#39;] DGS10_monthly = OHLC_monthly[[&#39;close&#39;]] DGS10_monthly.columns = [&#39;DGS10_monthly&#39;] . len(DGS10_weekly) . 522 . len(DGS10_monthly) . 121 . The ACF and PACF for daily, weekly, monthly series . Plot the ACF (auto-correlation function) and PACF (partial auto-correlation function) for each periodicity. . def acf_pacf_plot(daily,weekly,monthly): fig, ax = plt.subplots(2,3, figsize=(16,10)) sm.graphics.tsa.plot_acf(daily.values.squeeze(), title = list(daily.columns)[0], ax=ax[0,0]) ax[0,0].set_ylabel(&#39;ACF&#39;) sm.graphics.tsa.plot_acf(weekly.values.squeeze(), title = list(weekly.columns)[0], ax=ax[0,1]) ax[0,1].set_ylabel(&#39;ACF&#39;) sm.graphics.tsa.plot_acf(monthly.values.squeeze(), title = list(monthly.columns)[0], ax=ax[0,2]) ax[0,2].set_ylabel(&#39;ACF&#39;) sm.graphics.tsa.plot_pacf(daily.values.squeeze(), title = list(daily.columns)[0], ax=ax[1,0]) ax[1,0].set_ylabel(&#39;Partial ACF&#39;) sm.graphics.tsa.plot_pacf(weekly.values.squeeze(), title = list(weekly.columns)[0], ax=ax[1,1]) ax[1,1].set_ylabel(&#39;Partial ACF&#39;) sm.graphics.tsa.plot_pacf(monthly.values.squeeze(), title = list(monthly.columns)[0], ax=ax[1,2]) ax[1,2].set_ylabel(&#39;Partial ACF&#39;); . acf_pacf_plot(DGS10_daily, DGS10_weekly, DGS10_monthly) . The high first-order auto-correlation suggests that the time series has a unit root on every periodicity (daily, weekly and monthly). . Conduct Augmented Dickey-Fuller Test for Unit Roots . It is essential to determine whether the time series is &quot;stationary&quot;. Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostics tests. . We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. . If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $ Delta y_t$. . For each periodicity, apply the function adfuller() twice: . to the un-differenced series (null hypothesis: input series has a unit root) | to the first-differenced series (same null hypothesis about differenced series) | . Results for the un-differenced series: . DGS10_weekly . DGS10_weekly . Date . 2011-10-23 2.23 | . 2011-10-30 2.34 | . 2011-11-06 2.06 | . 2011-11-13 2.04 | . 2011-11-20 2.01 | . ... ... | . 2021-09-19 1.37 | . 2021-09-26 1.47 | . 2021-10-03 1.48 | . 2021-10-10 1.61 | . 2021-10-17 1.59 | . 522 rows × 1 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(sm.tsa.adfuller(DGS10_daily[&#39;DGS10_daily&#39;])[1]) . 0.38133527878417517 . print(sm.tsa.adfuller(DGS10_weekly[&#39;DGS10_weekly&#39;])[1]) . 0.4663384684155049 . print(sm.tsa.adfuller(DGS10_monthly[&#39;DGS10_monthly&#39;])[1]) . 0.42416021367886364 . For each periodicity, the null hypothesis of a unit root for the time series DGS10 is not rejected at the 0.05 level. The p-value for each test does not fall below standard critical values of 0.05 or 0.01. The p-value is the probability (assuming the null hypothesis is true) of realizing a test statistic as extreme as that computed for the input series. Smaller values (i.e., lower probabilities) provide stronger evidence against the null hyptohesis. The p-value decreases as the periodicity of the data shortens. This suggests that the time-series structure in the series DGS10 may be stronger at higher frequencies. . Results for the first-differenced series: . print(sm.tsa.adfuller((DGS10_daily.shift(1)-DGS10_daily).dropna()[&#39;DGS10_daily&#39;])[1]) . 0.0 . print(sm.tsa.adfuller((DGS10_weekly.shift(1)-DGS10_weekly).dropna()[&#39;DGS10_weekly&#39;])[1]) . 0.0 . print(sm.tsa.adfuller((DGS10_monthly.shift(1)-DGS10_monthly).dropna()[&#39;DGS10_monthly&#39;])[1]) . 1.5067346413036143e-17 . For each of the three time periodicities, the ADF test rejects the null hypothesis that a unit root is present for the first-differenced series. . The ACF and PACF for the differenced series of each periodicity . One application of the operator $(1 − B)$ produces a new series $ {Y_t }$ with no obvious deviations from stationarity. . diff_DGS10_daily = (DGS10_daily.shift(1)-DGS10_daily).dropna() diff_DGS10_daily.columns = [&#39;diff_DGS10_daily&#39;] diff_DGS10_weekly = (DGS10_weekly.shift(1)-DGS10_weekly).dropna() diff_DGS10_weekly.columns = [&#39;diff_DGS10_weekly&#39;] diff_DGS10_monthly = (DGS10_monthly.shift(1)-DGS10_monthly).dropna() diff_DGS10_monthly.columns = [&#39;diff_DGS10_monthly&#39;] . acf_pacf_plot(diff_DGS10_daily, diff_DGS10_weekly, diff_DGS10_monthly) . The apparent time series structure of DGS10 varies with the periodicity: . Daily: . strong negative order-7 autocorrelation and partial autocorrelation strong positive order-30 autocorrelation and partial autocorrelation . Weekly: . strong negative order-1 autocorrelation and partial autocorrelation strong positive order-26 autocorrelation and partial autocorrelation . Monthly: . strong negative order-19 autocorrelation and partial autocorrelation. . fig0 = px.line(DGS10_monthly, y=&#39;DGS10_monthly&#39;, height=400) fig0.show(); fig1 = px.line(diff_DGS10_monthly, y=&#39;diff_DGS10_monthly&#39;, height=300) fig1.show(); . . . . . The differenced series diff_DGS10_monthly crosses the level 0.0 many times over the historical period. There does not appear to be a tendency for the differenced series to stay below (or above) the zero level. The series appears consistent with covariance-stationary time series structure but whether the structure is other than white noise can be evaluated by evaluating AR(p) models for p = 0, 1, 2, ... and determining whether an AR(p) model for p &gt; 0 is identified as better than an AR(0), i.e., white noise. . The best AR(p) model for monthly data using the AIC criterion . warnings.filterwarnings(&quot;ignore&quot;) # Define the d and q parameters to take any value between 0 and 1 p = range(0, 25) AIC = [] AR_model = [] for param in p: try: model = AutoReg(diff_DGS10_monthly.values, param) results = model.fit() print(&#39;AR({}) - AIC:{}&#39;.format(param, results.aic), end=&#39; r&#39;) AIC.append(results.aic) AR_model.append([param]) except: continue . AR(24) - AIC:-2.912416394697231 . print(&#39;The smallest AIC is {} for model AR({})&#39;.format(min(AIC), AR_model[AIC.index(min(AIC))][0])) . The smallest AIC is -3.2368803775598853 for model AR(0) . model = AutoReg(diff_DGS10_monthly.values, 0) results = model.fit() . print(results.summary()) . AutoReg Model Results ============================================================================== Dep. Variable: y No. Observations: 120 Model: AutoReg(0) Log Likelihood 25.940 Method: Conditional MLE S.D. of innovations 0.195 Date: Mon, 14 Feb 2022 AIC -3.237 Time: 11:41:43 BIC -3.190 Sample: 0 HQIC -3.218 120 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] intercept 0.0048 0.018 0.272 0.786 -0.030 0.040 ============================================================================== . results.plot_diagnostics(lags=40, figsize=(16, 9)) plt.show() . In the plots above, we can observe that the residuals are uncorrelated (bottom right plot) and do not exhibit any obvious seasonality (the top left plot). Also, the residuals and roughly normally distributed with zero mean (top right plot). The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) roghly follows the linear trend of samples taken from a standard normal distribution with $N(0, 1)$. Again, this is a strong indication that the residuals are normally distributed. . We conclud that the best model for differenced data is AR(0), i.e., white noise. Thus for the original data the model is $X_t = 0.0048 + X_{t-1}+Z_t$, where $Z_t sim WN(0, sigma^2)$. The parameter $ sigma^2$ may be estimated by equating the sample ACVF with the model ACVF at lag 0. . sm.tsa.stattools.acovf(diff_DGS10_monthly.values, nlag=0) . array([0.03799831]) . Using the approximate solution $ sigma^2 = 0.04$, we obtain the following model . $$ X_t = 0.0048 + X_{t-1}+Z_t, Z_t sim WN(0,0.04). $$ The best AR(p) model for weekly data . warnings.filterwarnings(&quot;ignore&quot;) # Define the p parameter to take any value between 0 and 25 p = range(0, 25) AIC = [] AR_model = [] for param in p: try: model = AutoReg(diff_DGS10_weekly.values, param) results = model.fit() print(&#39;AR({}) - AIC:{}&#39;.format(param, results.aic), end=&#39; r&#39;) AIC.append(results.aic) AR_model.append([param]) except: continue . . print(&#39;The smallest AIC is {} for model AR({})&#39; .format(min(AIC), AR_model[AIC.index(min(AIC))][0])) . The smallest AIC is -4.624547948917449 for model AR(2) . model = AutoReg(diff_DGS10_weekly.values, 2) results = model.fit() . print(results.summary()) . AutoReg Model Results ============================================================================== Dep. Variable: y No. Observations: 521 Model: AutoReg(2) Log Likelihood 467.641 Method: Conditional MLE S.D. of innovations 0.098 Date: Mon, 14 Feb 2022 AIC -4.625 Time: 11:41:46 BIC -4.592 Sample: 2 HQIC -4.612 521 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] intercept 0.0010 0.004 0.230 0.818 -0.007 0.009 y.L1 -0.1028 0.044 -2.361 0.018 -0.188 -0.017 y.L2 0.0392 0.044 0.900 0.368 -0.046 0.125 Roots ============================================================================= Real Imaginary Modulus Frequency -- AR.1 -3.9057 +0.0000j 3.9057 0.5000 AR.2 6.5267 +0.0000j 6.5267 0.0000 -- . results.plot_diagnostics(lags=40, figsize=(16, 9)); . The residuals are consistent with their expected behavior under the model. . Evaluating the stationarity and cyclicality of the fitted AR(2) model to weekly data . To show the stationarity we have to show that all roots of characteristic polynomial lie outside the unit circle . $$ phi(z) = 1- phi_1 z- phi_2 z^2 neq 0 text{for all} |z|=1. $$From summarize of the Auto Regression model results we have estimates $ hat phi_1 = -0.1$ and $ hat phi_2 = 0.04$. . phi_1 = -0.1 phi_2 = 0.04 . np.polynomial.polynomial.polyroots((1, phi_1, phi_2)) . array([1.25-4.84122918j, 1.25+4.84122918j]) . Both roots are complex located outside the unit circle, we conclud that the fitted model is stationary. . # The following computation computes the period as it is determined by the # coefficients of the characteristic polynomial. twopif=np.arccos( abs(results.params[1])/(2*np.sqrt(results.params[2]))) f=twopif/(8*np.arctan(1)) period=-1/f print(period) . -4.802817275323328 . . Yule–Walker estimator for $ sigma^2$: $$ hat sigma^2 = hat gamma(0)- hat phi_1 hat gamma(1)- hat phi_2 hat gamma(2) $$ . sigma = sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2)[0] - phi_1*sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2)[1] - phi_2*sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2)[2] . print(&#39;sigma^2=&#39;, sigma) . sigma^2= 0.009790649932310859 . Finally we conclude for differenced weekly times series . $$ Y_t =0.001 -0.1Y_{t-1}+0.04Y_{t-2} + Z_t, Z_t sim WN(0,0.01) $$and for weekly time series . $$ (1+0.1B-0.04B^2)(1-B)X_t =0.001 + Z_{t}, Z_t sim WN(0,0.01). $$",
            "url": "https://ddemoivre.github.io/blog/jupyter/2022/02/13/_02_09_Time_series_analysis.html",
            "relUrl": "/jupyter/2022/02/13/_02_09_Time_series_analysis.html",
            "date": " • Feb 13, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Bird spaces classification",
            "content": ". gpu_info = !nvidia-smi gpu_info = &#39; n&#39;.join(gpu_info) if gpu_info.find(&#39;failed&#39;) &gt;= 0: print(&#39;Not connected to a GPU&#39;) else: print(gpu_info) . Mon Feb 7 16:45:36 2022 +--+ | NVIDIA-SMI 460.32.03 Driver Version: 460.32.03 CUDA Version: 11.2 | |-+-+-+ | GPU Name Persistence-M| Bus-Id Disp.A | Volatile Uncorr. ECC | | Fan Temp Perf Pwr:Usage/Cap| Memory-Usage | GPU-Util Compute M. | | | | MIG M. | |===============================+======================+======================| | 0 Tesla P100-PCIE... Off | 00000000:00:04.0 Off | 0 | | N/A 48C P0 35W / 250W | 4207MiB / 16280MiB | 0% Default | | | | N/A | +-+-+-+ +--+ | Processes: | | GPU GI CI PID Type Process name GPU Memory | | ID ID Usage | |=============================================================================| +--+ . from google.colab import drive drive.mount(&#39;/content/drive&#39;) . Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(&#34;/content/drive&#34;, force_remount=True). . import os os.environ[&#39;KAGGLE_CONFIG_DIR&#39;] = &quot;/content/drive/MyDrive/Kaggle&quot; . %cd /content/drive/MyDrive/Kaggle . /content/drive/MyDrive/Kaggle . . . import torch import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from fastai.vision.all import * #from nbdev.showdoc import * set_seed(2) %matplotlib inline . path = Path(&#39;/content/drive/MyDrive/Kaggle&#39;) . bs = 16 # uncomment this line if you run out of memory even after clicking Kernel-&gt;Restart . birds = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=parent_label, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = birds.dataloaders(path, valid_pct=0.2) . dls.show_batch(max_n=9, figsize=(9,6)) . print(dls.vocab) len(dls.vocab),dls.c . [&#39;AFRICAN CROWNED CRANE&#39;, &#39;AFRICAN FIREFINCH&#39;, &#39;ALBATROSS&#39;, &#39;ALEXANDRINE PARAKEET&#39;, &#39;AMERICAN AVOCET&#39;, &#39;AMERICAN BITTERN&#39;, &#39;AMERICAN COOT&#39;, &#39;AMERICAN GOLDFINCH&#39;, &#39;AMERICAN KESTREL&#39;, &#39;AMERICAN PIPIT&#39;, &#39;AMERICAN REDSTART&#39;, &#39;ANHINGA&#39;, &#39;ANNAS HUMMINGBIRD&#39;, &#39;ANTBIRD&#39;, &#39;ARARIPE MANAKIN&#39;, &#39;ASIAN CRESTED IBIS&#39;, &#39;BALD EAGLE&#39;, &#39;BALD IBIS&#39;, &#39;BALI STARLING&#39;, &#39;BALTIMORE ORIOLE&#39;, &#39;BANANAQUIT&#39;, &#39;BANDED BROADBILL&#39;, &#39;BANDED PITA&#39;, &#39;BAR-TAILED GODWIT&#39;, &#39;BARN OWL&#39;, &#39;BARN SWALLOW&#39;, &#39;BARRED PUFFBIRD&#39;, &#39;BAY-BREASTED WARBLER&#39;, &#39;BEARDED BARBET&#39;, &#39;BEARDED BELLBIRD&#39;, &#39;BEARDED REEDLING&#39;, &#39;BELTED KINGFISHER&#39;, &#39;BIRD OF PARADISE&#39;, &#39;BLACK &amp; YELLOW bROADBILL&#39;, &#39;BLACK BAZA&#39;, &#39;BLACK FRANCOLIN&#39;, &#39;BLACK SKIMMER&#39;, &#39;BLACK SWAN&#39;, &#39;BLACK TAIL CRAKE&#39;, &#39;BLACK THROATED BUSHTIT&#39;, &#39;BLACK THROATED WARBLER&#39;, &#39;BLACK VULTURE&#39;, &#39;BLACK-CAPPED CHICKADEE&#39;, &#39;BLACK-NECKED GREBE&#39;, &#39;BLACK-THROATED SPARROW&#39;, &#39;BLACKBURNIAM WARBLER&#39;, &#39;BLONDE CRESTED WOODPECKER&#39;, &#39;BLUE COAU&#39;, &#39;BLUE GROUSE&#39;, &#39;BLUE HERON&#39;, &#39;BLUE THROATED TOUCANET&#39;, &#39;BOBOLINK&#39;, &#39;BORNEAN BRISTLEHEAD&#39;, &#39;BORNEAN LEAFBIRD&#39;, &#39;BORNEAN PHEASANT&#39;, &#39;BRANDT CORMARANT&#39;, &#39;BROWN CREPPER&#39;, &#39;BROWN NOODY&#39;, &#39;BROWN THRASHER&#39;, &#39;BULWERS PHEASANT&#39;, &#39;CACTUS WREN&#39;, &#39;CALIFORNIA CONDOR&#39;, &#39;CALIFORNIA GULL&#39;, &#39;CALIFORNIA QUAIL&#39;, &#39;CANARY&#39;, &#39;CAPE GLOSSY STARLING&#39;, &#39;CAPE MAY WARBLER&#39;, &#39;CAPPED HERON&#39;, &#39;CAPUCHINBIRD&#39;, &#39;CARMINE BEE-EATER&#39;, &#39;CASPIAN TERN&#39;, &#39;CASSOWARY&#39;, &#39;CEDAR WAXWING&#39;, &#39;CERULEAN WARBLER&#39;, &#39;CHARA DE COLLAR&#39;, &#39;CHESTNET BELLIED EUPHONIA&#39;, &#39;CHIPPING SPARROW&#39;, &#39;CHUKAR PARTRIDGE&#39;, &#39;CINNAMON TEAL&#39;, &#39;CLARKS NUTCRACKER&#39;, &#39;COCK OF THE ROCK&#39;, &#39;COCKATOO&#39;, &#39;COLLARED ARACARI&#39;, &#39;COMMON FIRECREST&#39;, &#39;COMMON GRACKLE&#39;, &#39;COMMON HOUSE MARTIN&#39;, &#39;COMMON LOON&#39;, &#39;COMMON POORWILL&#39;, &#39;COMMON STARLING&#39;, &#39;COUCHS KINGBIRD&#39;, &#39;CRESTED AUKLET&#39;, &#39;CRESTED CARACARA&#39;, &#39;CRESTED NUTHATCH&#39;, &#39;CRIMSON SUNBIRD&#39;, &#39;CROW&#39;, &#39;CROWNED PIGEON&#39;, &#39;CUBAN TODY&#39;, &#39;CUBAN TROGON&#39;, &#39;CURL CRESTED ARACURI&#39;, &#39;D-ARNAUDS BARBET&#39;, &#39;DARK EYED JUNCO&#39;, &#39;DOUBLE BARRED FINCH&#39;, &#39;DOUBLE BRESTED CORMARANT&#39;, &#39;DOWNY WOODPECKER&#39;, &#39;EASTERN BLUEBIRD&#39;, &#39;EASTERN MEADOWLARK&#39;, &#39;EASTERN ROSELLA&#39;, &#39;EASTERN TOWEE&#39;, &#39;ELEGANT TROGON&#39;, &#39;ELLIOTS PHEASANT&#39;, &#39;EMPEROR PENGUIN&#39;, &#39;EMU&#39;, &#39;ENGGANO MYNA&#39;, &#39;EURASIAN GOLDEN ORIOLE&#39;, &#39;EURASIAN MAGPIE&#39;, &#39;EVENING GROSBEAK&#39;, &#39;FAIRY BLUEBIRD&#39;, &#39;FIRE TAILLED MYZORNIS&#39;, &#39;FLAME TANAGER&#39;, &#39;FLAMINGO&#39;, &#39;FRIGATE&#39;, &#39;GAMBELS QUAIL&#39;, &#39;GANG GANG COCKATOO&#39;, &#39;GILA WOODPECKER&#39;, &#39;GILDED FLICKER&#39;, &#39;GLOSSY IBIS&#39;, &#39;GO AWAY BIRD&#39;, &#39;GOLD WING WARBLER&#39;, &#39;GOLDEN CHEEKED WARBLER&#39;, &#39;GOLDEN CHLOROPHONIA&#39;, &#39;GOLDEN EAGLE&#39;, &#39;GOLDEN PHEASANT&#39;, &#39;GOLDEN PIPIT&#39;, &#39;GOULDIAN FINCH&#39;, &#39;GRAY CATBIRD&#39;, &#39;GRAY KINGBIRD&#39;, &#39;GRAY PARTRIDGE&#39;, &#39;GREAT GRAY OWL&#39;, &#39;GREAT KISKADEE&#39;, &#39;GREAT POTOO&#39;, &#39;GREATOR SAGE GROUSE&#39;, &#39;GREEN BROADBILL&#39;, &#39;GREEN JAY&#39;, &#39;GREEN MAGPIE&#39;, &#39;GREY PLOVER&#39;, &#39;GROVED BILLED ANI&#39;, &#39;GUINEA TURACO&#39;, &#39;GUINEAFOWL&#39;, &#39;GYRFALCON&#39;, &#39;HARLEQUIN DUCK&#39;, &#39;HARPY EAGLE&#39;, &#39;HAWAIIAN GOOSE&#39;, &#39;HELMET VANGA&#39;, &#39;HIMALAYAN MONAL&#39;, &#39;HOATZIN&#39;, &#39;HOODED MERGANSER&#39;, &#39;HOOPOES&#39;, &#39;HORNBILL&#39;, &#39;HORNED GUAN&#39;, &#39;HORNED LARK&#39;, &#39;HORNED SUNGEM&#39;, &#39;HOUSE FINCH&#39;, &#39;HOUSE SPARROW&#39;, &#39;HYACINTH MACAW&#39;, &#39;IMPERIAL SHAQ&#39;, &#39;INCA TERN&#39;, &#39;INDIAN BUSTARD&#39;, &#39;INDIAN PITTA&#39;, &#39;INDIAN ROLLER&#39;, &#39;INDIGO BUNTING&#39;, &#39;IWI&#39;, &#39;JABIRU&#39;, &#39;JAVA SPARROW&#39;, &#39;KAGU&#39;, &#39;KAKAPO&#39;, &#39;KILLDEAR&#39;, &#39;KING VULTURE&#39;, &#39;KIWI&#39;, &#39;KOOKABURRA&#39;, &#39;LARK BUNTING&#39;, &#39;LAZULI BUNTING&#39;, &#39;LILAC ROLLER&#39;, &#39;LONG-EARED OWL&#39;, &#39;MAGPIE GOOSE&#39;, &#39;MALABAR HORNBILL&#39;, &#39;MALACHITE KINGFISHER&#39;, &#39;MALAGASY WHITE EYE&#39;, &#39;MALEO&#39;, &#39;MALLARD DUCK&#39;, &#39;MANDRIN DUCK&#39;, &#39;MANGROVE CUCKOO&#39;, &#39;MARABOU STORK&#39;, &#39;MASKED BOOBY&#39;, &#39;MASKED LAPWING&#39;, &#39;MIKADO PHEASANT&#39;, &#39;MOURNING DOVE&#39;, &#39;MYNA&#39;, &#39;NICOBAR PIGEON&#39;, &#39;NOISY FRIARBIRD&#39;, &#39;NORTHERN CARDINAL&#39;, &#39;NORTHERN FLICKER&#39;, &#39;NORTHERN FULMAR&#39;, &#39;NORTHERN GANNET&#39;, &#39;NORTHERN GOSHAWK&#39;, &#39;NORTHERN JACANA&#39;, &#39;NORTHERN MOCKINGBIRD&#39;, &#39;NORTHERN PARULA&#39;, &#39;NORTHERN RED BISHOP&#39;, &#39;NORTHERN SHOVELER&#39;, &#39;OCELLATED TURKEY&#39;, &#39;OKINAWA RAIL&#39;, &#39;ORANGE BRESTED BUNTING&#39;, &#39;ORIENTAL BAY OWL&#39;, &#39;OSPREY&#39;, &#39;OSTRICH&#39;, &#39;OVENBIRD&#39;, &#39;OYSTER CATCHER&#39;, &#39;PAINTED BUNTIG&#39;, &#39;PALILA&#39;, &#39;PARADISE TANAGER&#39;, &#39;PARAKETT AKULET&#39;, &#39;PARUS MAJOR&#39;, &#39;PATAGONIAN SIERRA FINCH&#39;, &#39;PEACOCK&#39;, &#39;PELICAN&#39;, &#39;PEREGRINE FALCON&#39;, &#39;PHILIPPINE EAGLE&#39;, &#39;PINK ROBIN&#39;, &#39;POMARINE JAEGER&#39;, &#39;PUFFIN&#39;, &#39;PURPLE FINCH&#39;, &#39;PURPLE GALLINULE&#39;, &#39;PURPLE MARTIN&#39;, &#39;PURPLE SWAMPHEN&#39;, &#39;PYGMY KINGFISHER&#39;, &#39;QUETZAL&#39;, &#39;RAINBOW LORIKEET&#39;, &#39;RAZORBILL&#39;, &#39;RED BEARDED BEE EATER&#39;, &#39;RED BELLIED PITTA&#39;, &#39;RED BROWED FINCH&#39;, &#39;RED FACED CORMORANT&#39;, &#39;RED FACED WARBLER&#39;, &#39;RED FODY&#39;, &#39;RED HEADED DUCK&#39;, &#39;RED HEADED WOODPECKER&#39;, &#39;RED HONEY CREEPER&#39;, &#39;RED NAPED TROGON&#39;, &#39;RED TAILED HAWK&#39;, &#39;RED TAILED THRUSH&#39;, &#39;RED WINGED BLACKBIRD&#39;, &#39;RED WISKERED BULBUL&#39;, &#39;REGENT BOWERBIRD&#39;, &#39;RING-NECKED PHEASANT&#39;, &#39;ROADRUNNER&#39;, &#39;ROBIN&#39;, &#39;ROCK DOVE&#39;, &#39;ROSY FACED LOVEBIRD&#39;, &#39;ROUGH LEG BUZZARD&#39;, &#39;ROYAL FLYCATCHER&#39;, &#39;RUBY THROATED HUMMINGBIRD&#39;, &#39;RUDY KINGFISHER&#39;, &#39;RUFOUS KINGFISHER&#39;, &#39;RUFUOS MOTMOT&#39;, &#39;SAMATRAN THRUSH&#39;, &#39;SAND MARTIN&#39;, &#39;SANDHILL CRANE&#39;, &#39;SATYR TRAGOPAN&#39;, &#39;SCARLET CROWNED FRUIT DOVE&#39;, &#39;SCARLET IBIS&#39;, &#39;SCARLET MACAW&#39;, &#39;SCARLET TANAGER&#39;, &#39;SHOEBILL&#39;, &#39;SHORT BILLED DOWITCHER&#39;, &#39;SMITHS LONGSPUR&#39;, &#39;SNOWY EGRET&#39;, &#39;SNOWY OWL&#39;, &#39;SORA&#39;, &#39;SPANGLED COTINGA&#39;, &#39;SPLENDID WREN&#39;, &#39;SPOON BILED SANDPIPER&#39;, &#39;SPOONBILL&#39;, &#39;SPOTTED CATBIRD&#39;, &#39;SRI LANKA BLUE MAGPIE&#39;, &#39;STEAMER DUCK&#39;, &#39;STORK BILLED KINGFISHER&#39;, &#39;STRAWBERRY FINCH&#39;, &#39;STRIPED OWL&#39;, &#39;STRIPPED MANAKIN&#39;, &#39;STRIPPED SWALLOW&#39;, &#39;SUPERB STARLING&#39;, &#39;SWINHOES PHEASANT&#39;, &#39;TAIWAN MAGPIE&#39;, &#39;TAKAHE&#39;, &#39;TASMANIAN HEN&#39;, &#39;TEAL DUCK&#39;, &#39;TIT MOUSE&#39;, &#39;TOUCHAN&#39;, &#39;TOWNSENDS WARBLER&#39;, &#39;TREE SWALLOW&#39;, &#39;TROPICAL KINGBIRD&#39;, &#39;TRUMPTER SWAN&#39;, &#39;TURKEY VULTURE&#39;, &#39;TURQUOISE MOTMOT&#39;, &#39;UMBRELLA BIRD&#39;, &#39;VARIED THRUSH&#39;, &#39;VENEZUELIAN TROUPIAL&#39;, &#39;VERMILION FLYCATHER&#39;, &#39;VICTORIA CROWNED PIGEON&#39;, &#39;VIOLET GREEN SWALLOW&#39;, &#39;VULTURINE GUINEAFOWL&#39;, &#39;WALL CREAPER&#39;, &#39;WATTLED CURASSOW&#39;, &#39;WHIMBREL&#39;, &#39;WHITE BROWED CRAKE&#39;, &#39;WHITE CHEEKED TURACO&#39;, &#39;WHITE NECKED RAVEN&#39;, &#39;WHITE TAILED TROPIC&#39;, &#39;WHITE THROATED BEE EATER&#39;, &#39;WILD TURKEY&#39;, &#39;WILSONS BIRD OF PARADISE&#39;, &#39;WOOD DUCK&#39;, &#39;YELLOW BELLIED FLOWERPECKER&#39;, &#39;YELLOW CACIQUE&#39;, &#39;YELLOW HEADED BLACKBIRD&#39;, &#39;images to test&#39;] . (326, 326) . &lt;&gt; shows the difference between an image that has been zoomed, interpolated, rotated, and then interpolated again (which is the approach used by all other deep learning libraries), shown here on the right, and an image that has been zoomed and rotated as one operation and then interpolated just once on the left (the fastai approach), shown here on the left. . Training: resnet34 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.281556 | 0.585761 | 0.146585 | 03:51 | . epoch train_loss valid_loss error_rate time . 0 | 0.611539 | 0.314773 | 0.080854 | 03:56 | . 1 | 0.308219 | 0.205119 | 0.050114 | 03:54 | . learn.model . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=326, bias=False) ) ) . Improving our model . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.lr_find(start_lr=1e-5, end_lr=1e1) . SuggestedLRs(valley=0.003311311127617955) . learn.fine_tune?? . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.091636 | 0.601378 | 0.163981 | 03:48 | . 1 | 0.630284 | 0.308223 | 0.079371 | 03:49 | . 2 | 0.422914 | 0.247530 | 0.061184 | 03:47 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(valley=7.585775892948732e-05) . learn.fit_one_cycle(10, lr_max=9e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.366718 | 0.241547 | 0.060986 | 03:55 | . 1 | 0.392569 | 0.264818 | 0.068202 | 03:57 | . 2 | 0.382351 | 0.240760 | 0.061085 | 03:54 | . 3 | 0.280636 | 0.220931 | 0.055056 | 03:55 | . 4 | 0.223109 | 0.198764 | 0.047346 | 03:54 | . 5 | 0.156686 | 0.174471 | 0.040130 | 03:54 | . 6 | 0.125461 | 0.163392 | 0.039439 | 03:54 | . 7 | 0.095606 | 0.158930 | 0.036276 | 03:54 | . 8 | 0.066164 | 0.154093 | 0.034200 | 03:54 | . 9 | 0.055492 | 0.152054 | 0.034398 | 03:54 | . learn.recorder.plot_loss() .",
            "url": "https://ddemoivre.github.io/blog/jupyter/2022/02/09/Image_classification.html",
            "relUrl": "/jupyter/2022/02/09/Image_classification.html",
            "date": " • Feb 9, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ddemoivre.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ddemoivre.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I’m mathematician, I blog about machin learnig and modeling. .",
          "url": "https://ddemoivre.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ddemoivre.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}