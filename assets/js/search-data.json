{
  
    
        "post0": {
            "title": "Tabular Modeling",
            "content": ". Modern machine learning can be distilled down to a couple of key techniques that are widely applicable. Recent studies have shown that the vast majority of datasets can be best modeled with just two methods: . Ensembles of decision trees (i.e., random forests and gradient boosting machines), mainly for structured data (such as you might find in a database table at most companies) | Multilayered neural networks learned with SGD (i.e., shallow and/or deep learning), mainly for unstructured data (such as audio, images, and natural language) | . Deep learning often gives superior resutls for unstructured data, but for many kinds of structured data these two approaches tend to give quite similar results. But ensembles of decision trees tend to train faster, are often easier to interpret, do not require special GPU hardware for inference at scale, and often require less hyperparameter tuning. . Most importantly, the critical step of interpreting a model of tabular data is significantly easier for decision tree ensembles. There are tools and methods for answering the pertinent questions, like: Which columns in the dataset were the most important for your predictions? How are they related to the dependent variable? How do they interact with each other? And which particular features were most important for some particular observation? . Therefore, ensembles of decision trees are our first approach for analyzing a new tabular dataset. . The Dataset . The dataset we use is the used cars dataset of Craigslist from Kaggle, is the world&#39;s largest collection of used vehicles for sale. . Look at the Data . df = pd.read_csv(&#39;vehicles.csv&#39;, low_memory=False) . df.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 426880 entries, 0 to 426879 Data columns (total 26 columns): # Column Non-Null Count Dtype -- -- 0 id 426880 non-null int64 1 url 426880 non-null object 2 region 426880 non-null object 3 region_url 426880 non-null object 4 price 426880 non-null int64 5 year 425675 non-null float64 6 manufacturer 409234 non-null object 7 model 421603 non-null object 8 condition 252776 non-null object 9 cylinders 249202 non-null object 10 fuel 423867 non-null object 11 odometer 422480 non-null float64 12 title_status 418638 non-null object 13 transmission 424324 non-null object 14 VIN 265838 non-null object 15 drive 296313 non-null object 16 size 120519 non-null object 17 type 334022 non-null object 18 paint_color 296677 non-null object 19 image_url 426812 non-null object 20 description 426810 non-null object 21 county 0 non-null float64 22 state 426880 non-null object 23 lat 420331 non-null float64 24 long 420331 non-null float64 25 posting_date 426812 non-null object dtypes: float64(5), int64(2), object(19) memory usage: 84.7+ MB . drop_columns = [&#39;id&#39;, &#39;url&#39;, &#39;region_url&#39;, &#39;image_url&#39;, &#39;description&#39;, &#39;size&#39;, &#39;county&#39;, &#39;posting_date&#39;, &#39;VIN&#39;, &#39;paint_color&#39;] df = df.drop(columns = drop_columns) . At this point, a good next step is to handle ordinal columns. This refers to columns containing strings or similar, but where those strings have a natural ordering. For instance, here are the levels of conditions: . df[&#39;condition&#39;].unique() . array([nan, &#39;good&#39;, &#39;excellent&#39;, &#39;fair&#39;, &#39;like new&#39;, &#39;new&#39;, &#39;salvage&#39;], dtype=object) . We can tell Pandas about a suitable ordering of these levels like so: . conditions = &#39;new&#39;, &#39;like new&#39;, &#39;excellent&#39;, &#39;good&#39;, &#39;fair&#39;, &#39;salvage&#39; . df[&#39;condition&#39;] = df[&#39;condition&#39;].astype(&#39;category&#39;) df[&#39;condition&#39;].cat.set_categories(conditions, ordered=True, inplace=True) . /usr/local/lib/python3.7/dist-packages/pandas/core/arrays/categorical.py:2631: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object. res = method(*args, **kwargs) . df[&#39;cylinders&#39;].unique() . array([nan, &#39;8 cylinders&#39;, &#39;6 cylinders&#39;, &#39;4 cylinders&#39;, &#39;5 cylinders&#39;, &#39;other&#39;, &#39;3 cylinders&#39;, &#39;10 cylinders&#39;, &#39;12 cylinders&#39;], dtype=object) . cylinders = &#39;12 cylinders&#39;, &#39;10 cylinders&#39;, &#39;8 cylinders&#39;, &#39;6 cylinders&#39;, &#39;5 cylinders&#39;, &#39;4 cylinders&#39;, &#39;3 cylinders&#39;, &#39;other&#39; . (&#39;4 cylinders&#39;, &#39;3 cylinders&#39;, &#39;other&#39;) . df[&#39;cylinders&#39;] = df[&#39;cylinders&#39;].astype(&#39;category&#39;) df[&#39;cylinders&#39;].cat.set_categories(cylinders, ordered=True, inplace=True) . /usr/local/lib/python3.7/dist-packages/pandas/core/arrays/categorical.py:2631: FutureWarning: The `inplace` parameter in pandas.Categorical.set_categories is deprecated and will be removed in a future version. Removing unused categories will always return a new Categorical object. res = method(*args, **kwargs) . The most important data column is the dependent variable—that is, the one we want to predict. Recall that a model&#39;s metric is a function that reflects how good the predictions are. It&#39;s important to note what metric is being used for a project. Generally, selecting the metric is an important part of the project setup. . Let&#39;s look at price column. . df[df[&#39;price&#39;]&lt;10000][&#39;price&#39;].sample(n=1000, random_state=1).hist(bins=25); . In this case we will us root mean squared log error (RMSLE) between the actual and predicted prices. It is an extension on Mean Squared Error (MSE) that is mainly used when predictions have large deviations, which is the case with used cars prices. Values range from 0 up to thousands and we don&#39;t want to punish deviations in prediction as much as with MSE. . Note that whene price is equal to zero this means pheraps that the value of price is missing. We can see this on one example. . np.array(df.loc[10,:]) . array([&#39;el paso&#39;, 0, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, &#39;tx&#39;, nan, nan], dtype=object) . We need to do only a small amount of processing to use this: we take the log of the prices, so that rmse of that value will give us what we ultimately need: . dep_var = &#39;price&#39; . cond = df.price &gt; 0 df = df[cond] . try: df.loc[:,&#39;price&#39;] = np.log(df[&#39;price&#39;]) except SettingWithCopyError: pass . Decision trees . procs = [Categorify, FillMissing] . Categorify is a TabularProc that replaces a column with a numeric categorical column. FillMissing is a TabularProc that replaces missing values with the median of the column, and creates a new Boolean column that is set to True for any row where the value was missing. . We want to ensure that a model is able to predict the future. But it means that if we are going to have a useful validation set, we also want the validation set to be later in time than the training set. The Kaggle training data ends in 2022, so we will define a narrower training dataset which consists only of the Kaggle training data from before 2018, and we&#39;ll define a validation set consisting of data from after 2019. . df = df[df[&#39;year&#39;].notnull()] cond = (df.year&lt;2019) train_idx = np.where( cond)[0] valid_idx = np.where(~cond)[0] splits = (list(train_idx),list(valid_idx)) . cont,cat = cont_cat_split(df, 1, dep_var=dep_var) . to = TabularPandas(df, procs, cat, cont, y_names=dep_var, splits=splits) . A TabularPandas behaves a lot like a fastai Datasets object, including providing train and valid attributes: . len(to.train),len(to.valid) . (350038, 42774) . to.show(3) . save_pickle(&#39;to.pkl&#39;,to) . Creating the Decision Tree . xs,y = to.train.xs,to.train.y valid_xs,valid_y = to.valid.xs,to.valid.y . Now that our data is all numeric, and there are no missing values, we can create a decision tree: . m = DecisionTreeRegressor(max_leaf_nodes=4) m.fit(xs, y); . To keep it simple, we&#39;ve told sklearn to just create four leaf nodes. To see what it&#39;s learned, we can display the tree: . draw_tree(m, xs, size=10, leaves_parallel=True, precision=2) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; Tree 0 year ≤ 2011.5 squared_error = 1.51 samples = 350038 value = 9.31 1 fuel ≤ 1.5 squared_error = 1.15 samples = 151943 value = 8.82 0&#45;&gt;1 True 2 year ≤ 2014.5 squared_error = 1.47 samples = 198095 value = 9.69 0&#45;&gt;2 False 5 squared_error = 1.19 samples = 11838 value = 9.59 1&#45;&gt;5 6 squared_error = 1.09 samples = 140105 value = 8.76 1&#45;&gt;6 3 squared_error = 1.31 samples = 77523 value = 9.43 2&#45;&gt;3 4 squared_error = 1.5 samples = 120572 value = 9.85 2&#45;&gt;4 We can show the same information using Terence Parr&#39;s powerful dtreeviz library: . samp_idx = np.random.permutation(len(y))[:500] dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . /usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names &#34;X does not have valid feature names, but&#34; . G node1 node2 leaf5 node1-&gt;leaf5 leaf6 node1-&gt;leaf6 leaf3 node2-&gt;leaf3 leaf4 node2-&gt;leaf4 node0 node0-&gt;node1 &#8804; node0-&gt;node2 &gt; This shows a chart of the distribution of the data for each split point. We can clearly see that there&#39;s a problem with our year data: there are cars made in the year 1927, apparently! For modeling purposes, 1927 is fine, but as you can see this outlier makes visualization of the values we are interested in more difficult. So, let&#39;s replace it with 1980: . xs.loc[xs[&#39;year&#39;]&lt;1980, &#39;year&#39;] = 1980 valid_xs.loc[valid_xs[&#39;year&#39;]&lt;1980, &#39;year&#39;] = 1980 . That change makes the split much clearer in the tree visualization, even although it doesn&#39;t actually change the result of the model in any significant way. This is a great example of how resilient decision trees are to data issues! . m = DecisionTreeRegressor(max_leaf_nodes=4).fit(xs, y) dtreeviz(m, xs.iloc[samp_idx], y.iloc[samp_idx], xs.columns, dep_var, fontname=&#39;DejaVu Sans&#39;, scale=1.6, label_fontsize=10, orientation=&#39;LR&#39;) . /usr/local/lib/python3.7/dist-packages/sklearn/base.py:451: UserWarning: X does not have valid feature names, but DecisionTreeRegressor was fitted with feature names &#34;X does not have valid feature names, but&#34; . G node1 node2 leaf5 node1-&gt;leaf5 leaf6 node1-&gt;leaf6 leaf3 node2-&gt;leaf3 leaf4 node2-&gt;leaf4 node0 node0-&gt;node1 &#8804; node0-&gt;node2 &gt; Let&#39;s now have the decision tree algorithm build a bigger tree. Here, we are not passing in any stopping criteria such as max_leaf_nodes: . m = DecisionTreeRegressor() m.fit(xs, y); . def r_mse(pred,y): return round(math.sqrt(((pred-y)**2).mean()), 6) def m_rmse(m, xs, y): return r_mse(m.predict(xs), y) . m_rmse(m, xs, y) . 0.102045 . m_rmse(m, valid_xs, valid_y) . 1.131917 . It looks like we might be overfitting pretty badly. Here&#39;s why: . m.get_n_leaves(), len(xs) . (203088, 350038) . We&#39;ve got too many nodes! Indeed, sklearn&#39;s default settings allow it to continue splitting nodes until there is only one item in each leaf node. Let&#39;s change the stopping rule to tell sklearn to ensure every leaf node contains at least 25 auction records: . m = DecisionTreeRegressor(min_samples_leaf=25) m.fit(to.train.xs, to.train.y) m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.74457, 0.98132) . That looks much better. Let&#39;s check the number of leaves again: . m.get_n_leaves() . 10655 . Much more reasonable! Building a decision tree is a good way to create a model of our data. It is very flexible, since it can clearly handle nonlinear relationships and interactions between variables. . Random Forest . In essence a random forest is a model that averages the predictions of a large number of decision trees, which are generated by randomly varying various parameters that specify what data is used to train the tree and other tree parameters. Bagging is a particular approach to &quot;ensembling,&quot; or combining the results of multiple models together. To see how it works in practice, let&#39;s get started on creating our own random forest! . Creating a Random Forest . In the following function definition n_estimators defines the number of trees we want, max_samples defines how many rows to sample for training each tree, and max_features defines how many columns to sample at each split point (where 0.5 means &quot;take half the total number of columns&quot;). We can also specify when to stop splitting the tree nodes, effectively limiting the depth of the tree, by including the same min_samples_leaf parameter we used in the last section. Finally, we pass n_jobs=-1 to tell sklearn to use all our CPUs to build the trees in parallel. By creating a little function for this, we can more quickly try different variations in the rest of this chapter: . def rf(xs, y, n_estimators=40, max_samples=200_000, max_features=0.5, min_samples_leaf=5, **kwargs): return RandomForestRegressor(n_jobs=-1, n_estimators=n_estimators, max_samples=max_samples, max_features=max_features, min_samples_leaf=min_samples_leaf, oob_score=True).fit(xs, y) . m = rf(xs, y); . m_rmse(m, xs, y), m_rmse(m, valid_xs, valid_y) . (0.582435, 0.884202) . To see the impact of nestimators, let&#39;s get the predictions from each individual tree in our forest (these are in the estimators attribute): . import warnings warnings.filterwarnings(&quot;ignore&quot;) preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . r_mse(preds.mean(0), valid_y) . 0.884202 . plt.plot([r_mse(preds[:i+1].mean(0), valid_y) for i in range(40)]); . Model Interpretation . For tabular data, model interpretation is particularly important. For a given model, the things we are most likely to be interested in are: . How confident are we in our predictions using a particular row of data? | For predicting with a particular row of data, what were the most important factors, and how did they influence that prediction? | Which columns are the strongest predictors, which can we ignore? | Which columns are effectively redundant with each other, for purposes of prediction? | How do predictions vary, as we vary these columns? | . Tree Variance for Prediction Confidence . preds = np.stack([t.predict(valid_xs) for t in m.estimators_]) . preds.shape . (40, 42774) . preds_std = preds.std(0) . Here are the standard deviations for the predictions for the first five prices, that is, the first five rows of the validation set: . preds_std[:5] . array([0.12760513, 0.499672 , 0.89026325, 0.316399 , 0.06906902]) . Feature Importance . def rf_feat_importance(m, df): return pd.DataFrame({&#39;cols&#39;:df.columns, &#39;imp&#39;:m.feature_importances_} ).sort_values(&#39;imp&#39;, ascending=False) . fi = rf_feat_importance(m, xs) fi[:10] . cols imp . 14 year | 0.169779 | . 16 lat | 0.147254 | . 15 odometer | 0.146727 | . 17 long | 0.116942 | . 2 model | 0.061251 | . 3 condition | 0.055504 | . 8 drive | 0.054248 | . 5 fuel | 0.048203 | . 4 cylinders | 0.038406 | . 1 manufacturer | 0.037436 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; A plot of the feature importances shows the relative importances more clearly: . def plot_fi(fi): return fi.plot(&#39;cols&#39;, &#39;imp&#39;, &#39;barh&#39;, figsize=(12,7), legend=False) plot_fi(fi[:10]); . Removing Low-Importance Variables . It seems likely that we could use just a subset of the columns by removing the variables of low importance and still get good results. Let&#39;s try just keeping those with a feature importance greater than 0.005: . to_keep = fi[fi.imp&gt;0.005].cols len(to_keep) . 15 . xs_imp = xs[to_keep] valid_xs_imp = valid_xs[to_keep] . m = rf(xs_imp, y) . Removing Redundant Features . cluster_columns(xs_imp) . In this chart, the pairs of columns that are most similar are the ones that were merged together early, far from the &quot;root&quot; of the tree at the left. In our case it seams that there are no closely correlated features. . Partial Dependence . As we&#39;ve seen, the two most important predictors are year lat and odometer. We&#39;d like to understand the relationship between these predictors and sale price. It&#39;s a good idea to first check the count of values per category (provided by the Pandas value_counts method), to see how common each category is: . ax = valid_xs[&#39;lat&#39;].hist() . ax = valid_xs[&#39;year&#39;].hist() . Partial dependence plots try to answer the question: if a row varied on nothing other than the feature in question, how would it impact the dependent variable? . For instance, how does year impact sale price, all other things being equal? . from sklearn.inspection import plot_partial_dependence fig, ax = plt.subplots(figsize=(12, 4)) plot_partial_dependence(m, valid_xs_imp, [&#39;odometer&#39;, &#39;lat&#39;], grid_resolution=20, ax=ax); .",
            "url": "https://ddemoivre.github.io/blog/jupyter/2022/02/22/_02_23_Tabular_Modeling.html",
            "relUrl": "/jupyter/2022/02/22/_02_23_Tabular_Modeling.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Time Series Analysis of the US Treasury 10- Year Yield",
            "content": ". AR(p) Models . A time series model for the observed data $ {x_t }$ is a specification of the joint distribution (or possibly only the means and covariances) of a sequence of random variables $ {X_t }$ of which $ {x_t }$ is postulated to be a realization. . The causal autoregressive $AR(p)$ process is defined by $$ X_t- phi_1 X_{t-1} - ...- phi_p X_{t-p}=c + Z_t, {Z_t} sim WN(0, sigma^2). $$ . A time series $ {X_t }$ is (covariance) stationnary if the mean function $ mu_X(t):= E(X_t)$ is independent of $t$, and the autocovariance function (ACVF) of $ {X_t }$ at lag $h$ . $$ gamma_X(t+h,t) := Cov(X_{t+h},X_t) = mathbb{E}[(X_{t+h}- mu_X(t+h))(X_{t}- mu_X(t))] $$is independent of $t$ for each $h$. . To assess the degree of dependence in the data and to select a model for the data that reflects this, one of the important tools we use is the sample autocorrelation function (sample ACF) of the data. Let $ {X_t }$ be a stationary time series. The autocorrelation function of $ {X_t }$ at lag $h$ is . $$ rho_X(h):= frac{ gamma_X(h)}{ gamma_X(0)} = Cor(X_{t+h},X_{t}). $$Let $x_1,...,x_n$ be observations of a time series. The sample autocorrelation function is . $$ hat rho(h) = frac{ hat gamma(h)}{ hat gamma(0)}, $$where $ hat gamma(h)$ is the sample autocovariance function i.e., $ hat gamma(h): = n^{-1} sum_{t=1}^{n-|h|}(x_{n-|h|}- bar{x})(x_t- bar{x})$, for $ -n&lt;h&lt;n$ and $ bar{x}:=n^{-1} sum^n_{t=1} x_t$. . We define sample PACF in an analogous way. If we believe that the data are realized values of a stationary time series $ {X_t }$, then the sample ACF will provide us with an estimate of the ACF of $ {X_t }$. This estimate may suggest which of the many possible stationary time series models is a suitable candidate for representing the dependence in the data. For example, a sample ACF that is close to zero for all nonzero lags suggests that an appropriate model for the data might be iid noise. . A partial autocorrelation function (PACF) of an ARMA process $ {X_t }$ is the function $ alpha( cdot)$ defined by the equations . $$ alpha(0) = 1 text{and} alpha(h) = phi_{hh}, h geq 1, $$where $ phi_{hh}$ is the last component of $ Phi_h = Gamma^{-1}_{h} gamma_h$, $ Gamma_h$ is $h$-dimensional the covariance matrix and $ gamma_h = [ gamma(1), gamma(2),..., gamma(h)]&#39;$. . The partial autocorrelation function is a tool that exploits the fact that, whereas an $AR(p)$ process has an autocorrelation function that is infinite in extent, the partial autocorrelations are zero beyond lag $p$. We define sample PACF for observed data in an analogous way. . Reference . Peter J. Brockwell, Richard A. Davis, Introduction to time series and forecasting, third edition | . Load python libraries and Federal Reserve data . The following commands re-load the data and evaluates the presence and nature of missing values. . %matplotlib inline import warnings import numpy as np import pandas as pd import statsmodels.api as sm from statsmodels.formula.api import ols from statsmodels.tsa.ar_model import AutoReg from pylab import mpl, plt import plotly.express as px import plotly.graph_objects as go from plotly.subplots import make_subplots plt.style.use(&#39;seaborn&#39;) mpl.rcParams[&#39;font.family&#39;] = &#39;serif&#39; . fred_data = pd.read_csv(&quot;fred_data.csv&quot;, index_col=&quot;DATE&quot;) . fred_data.head() . DGS3MO DGS1 DGS5 DGS10 DAAA DBAA DCOILWTICO . DATE . 2011-10-17 0.04 | 0.12 | 1.08 | 2.18 | 4.00 | 5.44 | 86.38 | . 2011-10-18 0.04 | 0.12 | 1.07 | 2.19 | 3.96 | 5.42 | 88.34 | . 2011-10-19 0.03 | 0.11 | 1.05 | 2.18 | 3.97 | 5.39 | 86.11 | . 2011-10-20 0.03 | 0.12 | 1.07 | 2.20 | 3.98 | 5.40 | 86.07 | . 2011-10-21 0.02 | 0.12 | 1.08 | 2.23 | 3.99 | 5.41 | 87.19 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; . fig = px.line(fred_data, y=&#39;DGS10&#39;) fig.show(); . . . # Print out the counts of missing values fred_data.isna().sum() . DGS3MO 108 DGS1 108 DGS5 108 DGS10 108 DAAA 108 DBAA 108 DCOILWTICO 97 dtype: int64 . fred_data[fred_data.isna()[&quot;DGS10&quot;]==True].index . Index([&#39;2011-11-11&#39;, &#39;2011-11-24&#39;, &#39;2011-12-26&#39;, &#39;2012-01-02&#39;, &#39;2012-01-16&#39;, &#39;2012-02-20&#39;, &#39;2012-05-28&#39;, &#39;2012-07-04&#39;, &#39;2012-09-03&#39;, &#39;2012-10-08&#39;, ... &#39;2020-11-11&#39;, &#39;2020-11-26&#39;, &#39;2020-12-25&#39;, &#39;2021-01-01&#39;, &#39;2021-01-18&#39;, &#39;2021-02-15&#39;, &#39;2021-05-31&#39;, &#39;2021-07-05&#39;, &#39;2021-09-06&#39;, &#39;2021-10-11&#39;], dtype=&#39;object&#39;, name=&#39;DATE&#39;, length=108) . # in the bond market of the US. # Define fred_data0 as sub matrix with nonmissing data for DGS10 fred_data0 = fred_data[fred_data.isna()[&quot;DGS10&quot;]==False] . fred_data0.isna().sum() . DGS3MO 0 DGS1 0 DGS5 0 DGS10 0 DAAA 2 DBAA 2 DCOILWTICO 9 dtype: int64 . DGS10_daily = fred_data0[[&quot;DGS10&quot;]] . len(DGS10_daily) . 2502 . DGS10_daily.head() . DGS10 . DATE . 2011-10-17 2.18 | . 2011-10-18 2.19 | . 2011-10-19 2.18 | . 2011-10-20 2.20 | . 2011-10-21 2.23 | . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; Create weekly and monthly time series . # to an Open/High/Low/Close series on a periodicity lower than the input data object. # Plot OHLC chart which shows the open, high, low, and close price for a given period. warnings.filterwarnings(&quot;ignore&quot;) DGS10_daily[&#39;Date&#39;] = pd.to_datetime(DGS10_daily.index, format=&#39;%Y/%m/%d&#39;) DGS10_daily = DGS10_daily.set_index([&#39;Date&#39;]) DGS10_daily.columns = [&#39;DGS10_daily&#39;] def resample_plot(data, how): df = pd.DataFrame(columns=[&#39;open&#39;, &#39;high&#39;, &#39;low&#39;, &#39;close&#39;]) ohlc = {&#39;open&#39;: lambda x: x[0], &#39;high&#39;: max, &#39;low&#39;: min, &#39;close&#39;: lambda x: x[-1]} for key in ohlc.keys(): df[key] = data.resample(how).apply(ohlc[key]) fig = go.Figure(data=[go.Candlestick(x=df.index, open=df.loc[:,&#39;open&#39;], high=df.loc[:,&#39;high&#39;], low=df.loc[:,&#39;low&#39;], close=df.loc[:,&#39;close&#39;]) ]) f = lambda x: &#39;week&#39; if x==&#39;W&#39; else &#39;month&#39; fig.update_layout(title=&#39;OHLC chart for {}&#39;.format(f(how)), yaxis_title=&#39;DGS10&#39;, xaxis_rangeslider_visible=False) fig.show() return df . OHLC_weekly = resample_plot(DGS10_daily,&#39;W&#39;) . . . OHLC_monthly = resample_plot(DGS10_daily,&#39;M&#39;) . . . DGS10_weekly = OHLC_weekly[[&#39;close&#39;]] DGS10_weekly.columns = [&#39;DGS10_weekly&#39;] DGS10_monthly = OHLC_monthly[[&#39;close&#39;]] DGS10_monthly.columns = [&#39;DGS10_monthly&#39;] . len(DGS10_weekly) . 522 . len(DGS10_monthly) . 121 . The ACF and PACF for daily, weekly, monthly series . Plot the ACF (auto-correlation function) and PACF (partial auto-correlation function) for each periodicity. . def acf_pacf_plot(daily,weekly,monthly): fig, ax = plt.subplots(2,3, figsize=(16,10)) sm.graphics.tsa.plot_acf(daily.values.squeeze(), title = list(daily.columns)[0], ax=ax[0,0]) ax[0,0].set_ylabel(&#39;ACF&#39;) sm.graphics.tsa.plot_acf(weekly.values.squeeze(), title = list(weekly.columns)[0], ax=ax[0,1]) ax[0,1].set_ylabel(&#39;ACF&#39;) sm.graphics.tsa.plot_acf(monthly.values.squeeze(), title = list(monthly.columns)[0], ax=ax[0,2]) ax[0,2].set_ylabel(&#39;ACF&#39;) sm.graphics.tsa.plot_pacf(daily.values.squeeze(), title = list(daily.columns)[0], ax=ax[1,0]) ax[1,0].set_ylabel(&#39;Partial ACF&#39;) sm.graphics.tsa.plot_pacf(weekly.values.squeeze(), title = list(weekly.columns)[0], ax=ax[1,1]) ax[1,1].set_ylabel(&#39;Partial ACF&#39;) sm.graphics.tsa.plot_pacf(monthly.values.squeeze(), title = list(monthly.columns)[0], ax=ax[1,2]) ax[1,2].set_ylabel(&#39;Partial ACF&#39;); . acf_pacf_plot(DGS10_daily, DGS10_weekly, DGS10_monthly) . The high first-order auto-correlation suggests that the time series has a unit root on every periodicity (daily, weekly and monthly). . Conduct Augmented Dickey-Fuller Test for Unit Roots . It is essential to determine whether the time series is &quot;stationary&quot;. Informally, stationarity is when the auto-covariance is independent of time. Failure to establish stationarity will almost certainly lead to misinterpretation of model identification and diagnostics tests. . We perform an Augmented Dickey-Fuller test to establish stationarity under the assumption that the time series has a constant bias but does not exhibit a time trend. In other words, we assume that the time series is already de-trended. . If the stationarity test fails, even after first de-trending the time series, then one potential recourse is to simply take differences of time series and predict $ Delta y_t$. . For each periodicity, apply the function adfuller() twice: . to the un-differenced series (null hypothesis: input series has a unit root) | to the first-differenced series (same null hypothesis about differenced series) | . Results for the un-differenced series: . DGS10_weekly . DGS10_weekly . Date . 2011-10-23 2.23 | . 2011-10-30 2.34 | . 2011-11-06 2.06 | . 2011-11-13 2.04 | . 2011-11-20 2.01 | . ... ... | . 2021-09-19 1.37 | . 2021-09-26 1.47 | . 2021-10-03 1.48 | . 2021-10-10 1.61 | . 2021-10-17 1.59 | . 522 rows × 1 columns . &lt;svg xmlns=&quot;http://www.w3.org/2000/svg&quot; height=&quot;24px&quot;viewBox=&quot;0 0 24 24&quot; width=&quot;24px&quot;&gt; &lt;/svg&gt; print(sm.tsa.adfuller(DGS10_daily[&#39;DGS10_daily&#39;])[1]) . 0.38133527878417517 . print(sm.tsa.adfuller(DGS10_weekly[&#39;DGS10_weekly&#39;])[1]) . 0.4663384684155049 . print(sm.tsa.adfuller(DGS10_monthly[&#39;DGS10_monthly&#39;])[1]) . 0.42416021367886364 . For each periodicity, the null hypothesis of a unit root for the time series DGS10 is not rejected at the 0.05 level. The p-value for each test does not fall below standard critical values of 0.05 or 0.01. The p-value is the probability (assuming the null hypothesis is true) of realizing a test statistic as extreme as that computed for the input series. Smaller values (i.e., lower probabilities) provide stronger evidence against the null hyptohesis. The p-value decreases as the periodicity of the data shortens. This suggests that the time-series structure in the series DGS10 may be stronger at higher frequencies. . Results for the first-differenced series: . print(sm.tsa.adfuller((DGS10_daily.shift(1)-DGS10_daily).dropna()[&#39;DGS10_daily&#39;])[1]) . 0.0 . print(sm.tsa.adfuller((DGS10_weekly.shift(1)-DGS10_weekly).dropna()[&#39;DGS10_weekly&#39;])[1]) . 0.0 . print(sm.tsa.adfuller((DGS10_monthly.shift(1)-DGS10_monthly).dropna()[&#39;DGS10_monthly&#39;])[1]) . 1.5067346413036143e-17 . For each of the three time periodicities, the ADF test rejects the null hypothesis that a unit root is present for the first-differenced series. . The ACF and PACF for the differenced series of each periodicity . One application of the operator $(1 − B)$ produces a new series $ {Y_t }$ with no obvious deviations from stationarity. . diff_DGS10_daily = (DGS10_daily.shift(1)-DGS10_daily).dropna() diff_DGS10_daily.columns = [&#39;diff_DGS10_daily&#39;] diff_DGS10_weekly = (DGS10_weekly.shift(1)-DGS10_weekly).dropna() diff_DGS10_weekly.columns = [&#39;diff_DGS10_weekly&#39;] diff_DGS10_monthly = (DGS10_monthly.shift(1)-DGS10_monthly).dropna() diff_DGS10_monthly.columns = [&#39;diff_DGS10_monthly&#39;] . acf_pacf_plot(diff_DGS10_daily, diff_DGS10_weekly, diff_DGS10_monthly) . The apparent time series structure of DGS10 varies with the periodicity: . Daily: . strong negative order-7 autocorrelation and partial autocorrelation strong positive order-30 autocorrelation and partial autocorrelation . Weekly: . strong negative order-1 autocorrelation and partial autocorrelation strong positive order-26 autocorrelation and partial autocorrelation . Monthly: . strong negative order-19 autocorrelation and partial autocorrelation. . fig0 = px.line(DGS10_monthly, y=&#39;DGS10_monthly&#39;, height=400) fig0.show(); fig1 = px.line(diff_DGS10_monthly, y=&#39;diff_DGS10_monthly&#39;, height=300) fig1.show(); . . . . . The differenced series diff_DGS10_monthly crosses the level 0.0 many times over the historical period. There does not appear to be a tendency for the differenced series to stay below (or above) the zero level. The series appears consistent with covariance-stationary time series structure but whether the structure is other than white noise can be evaluated by evaluating AR(p) models for p = 0, 1, 2, ... and determining whether an AR(p) model for p &gt; 0 is identified as better than an AR(0), i.e., white noise. . The best AR(p) model for monthly data using the AIC criterion . warnings.filterwarnings(&quot;ignore&quot;) # Define the d and q parameters to take any value between 0 and 1 p = range(0, 25) AIC = [] AR_model = [] for param in p: try: model = AutoReg(diff_DGS10_monthly.values, param) results = model.fit() print(&#39;AR({}) - AIC:{}&#39;.format(param, results.aic), end=&#39; r&#39;) AIC.append(results.aic) AR_model.append([param]) except: continue . AR(24) - AIC:-2.912416394697231 . print(&#39;The smallest AIC is {} for model AR({})&#39;.format(min(AIC), AR_model[AIC.index(min(AIC))][0])) . The smallest AIC is -3.2368803775598853 for model AR(0) . model = AutoReg(diff_DGS10_monthly.values, 0) results = model.fit() . print(results.summary()) . AutoReg Model Results ============================================================================== Dep. Variable: y No. Observations: 120 Model: AutoReg(0) Log Likelihood 25.940 Method: Conditional MLE S.D. of innovations 0.195 Date: Wed, 09 Feb 2022 AIC -3.237 Time: 19:20:11 BIC -3.190 Sample: 0 HQIC -3.218 120 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] intercept 0.0048 0.018 0.272 0.786 -0.030 0.040 ============================================================================== . results.plot_diagnostics(lags=40, figsize=(16, 9)) plt.show() . In the plots above, we can observe that the residuals are uncorrelated (bottom right plot) and do not exhibit any obvious seasonality (the top left plot). Also, the residuals and roughly normally distributed with zero mean (top right plot). The qq-plot on the bottom left shows that the ordered distribution of residuals (blue dots) roghly follows the linear trend of samples taken from a standard normal distribution with $N(0, 1)$. Again, this is a strong indication that the residuals are normally distributed. . We conclud that the best model for differenced data is AR(0), i.e., white noise. Thus for the original data the model is $X_t = 0.0048 + X_{t-1}+Z_t$, where $Z_t sim WN(0, sigma^2)$. The parameter $ sigma^2$ may be estimated by equating the sample ACVF with the model ACVF at lag 0. . sm.tsa.stattools.acovf(diff_DGS10_monthly.values, nlag=0) . array([0.03799831]) . Using the approximate solution $ sigma^2 = 0.04$, we obtain the following model . $$ X_t = 0.0048 + X_{t-1}+Z_t, Z_t sim WN(0,0.04). $$ The best AR(p) model for weekly data . warnings.filterwarnings(&quot;ignore&quot;) # Define the p parameter to take any value between 0 and 25 p = range(0, 25) AIC = [] AR_model = [] for param in p: try: model = AutoReg(diff_DGS10_weekly.values, param) results = model.fit() print(&#39;AR({}) - AIC:{}&#39;.format(param, results.aic), end=&#39; r&#39;) AIC.append(results.aic) AR_model.append([param]) except: continue . AR(24) - AIC:-4.5746271365130635 . print(&#39;The smallest AIC is {} for model AR({})&#39; .format(min(AIC), AR_model[AIC.index(min(AIC))][0])) . The smallest AIC is -4.624547948917449 for model AR(2) . model = AutoReg(diff_DGS10_weekly.values, 2) results = model.fit() . print(results.summary()) . AutoReg Model Results ============================================================================== Dep. Variable: y No. Observations: 521 Model: AutoReg(2) Log Likelihood 467.641 Method: Conditional MLE S.D. of innovations 0.098 Date: Wed, 09 Feb 2022 AIC -4.625 Time: 19:20:12 BIC -4.592 Sample: 2 HQIC -4.612 521 ============================================================================== coef std err z P&gt;|z| [0.025 0.975] intercept 0.0010 0.004 0.230 0.818 -0.007 0.009 y.L1 -0.1028 0.044 -2.361 0.018 -0.188 -0.017 y.L2 0.0392 0.044 0.900 0.368 -0.046 0.125 Roots ============================================================================= Real Imaginary Modulus Frequency -- AR.1 -3.9057 +0.0000j 3.9057 0.5000 AR.2 6.5267 +0.0000j 6.5267 0.0000 -- . results.plot_diagnostics(lags=40, figsize=(16, 9)); . The residuals are consistent with their expected behavior under the model. . Evaluating the stationarity and cyclicality of the fitted AR(2) model to weekly data . To show the stationarity we have to show that all roots of characteristic polynomial lie outside the unit circle . $$ phi(z) = 1- phi_1 z- phi_2 z^2 neq 0 text{for all} |z|=1. $$From summarize of the Auto Regression model results we have estimates $ hat phi_1 = -0.1$ and $ hat phi_2 = 0.04$. . phi_1 = -0.1 phi_2 = 0.04 . np.polynomial.polynomial.polyroots((1, phi_1, phi_2)) . array([1.25-4.84122918j, 1.25+4.84122918j]) . Both roots are complex located outside the unit circle, we conclud that the fitted model is stationary. . # The following computation computes the period as it is determined by the # coefficients of the characteristic polynomial. twopif=np.arccos( abs(results.params[1])/(2*np.sqrt(results.params[2]))) f=twopif/(8*np.arctan(1)) period=-1/f print(period) . -4.802817275323328 . . Yule–Walker estimator for $ sigma^2$: $$ hat sigma^2 = hat gamma(0)- hat phi_1 hat gamma(1)- hat phi_2 hat gamma(2) $$ . sigma = sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2)[0] - phi_1*sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2)[1] - phi_2*sm.tsa.stattools.acovf(diff_DGS10_weekly.values, nlag=2)[2] . print(&#39;sigma^2=&#39;, sigma) . sigma^2= 0.009790649932310859 . Finally we conclude for differenced weekly times series . $$ Y_t =0.001 -0.1Y_{t-1}+0.04Y_{t-2} + Z_t, Z_t sim WN(0,0.01) $$and for weekly time series . $$ (1+0.1B-0.04B^2)(1-B)X_t =0.001 + Z_{t}, Z_t sim WN(0,0.01). $$",
            "url": "https://ddemoivre.github.io/blog/jupyter/2022/02/22/_02_09_Time_series_analysis.html",
            "relUrl": "/jupyter/2022/02/22/_02_09_Time_series_analysis.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Bird spaces classification",
            "content": ". import os os.environ[&#39;KAGGLE_CONFIG_DIR&#39;] = &quot;/content/drive/MyDrive/Kaggle&quot; . %cd /content/drive/MyDrive/Kaggle . /content/drive/MyDrive/Kaggle . import torch import numpy as np # linear algebra import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv) from fastai.vision.all import * #from nbdev.showdoc import * set_seed(2) %matplotlib inline . path = Path(&#39;/content/drive/MyDrive/Kaggle&#39;) . bs = 16 # uncomment this line if you run out of memory even after clicking Kernel-&gt;Restart . birds = DataBlock(blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=parent_label, item_tfms=Resize(460), batch_tfms=aug_transforms(size=224, min_scale=0.75)) dls = birds.dataloaders(path, valid_pct=0.2) . dls.show_batch(max_n=9, figsize=(9,6)) . print(dls.vocab) len(dls.vocab),dls.c . [&#39;AFRICAN CROWNED CRANE&#39;, &#39;AFRICAN FIREFINCH&#39;, &#39;ALBATROSS&#39;, &#39;ALEXANDRINE PARAKEET&#39;, &#39;AMERICAN AVOCET&#39;, &#39;AMERICAN BITTERN&#39;, &#39;AMERICAN COOT&#39;, &#39;AMERICAN GOLDFINCH&#39;, &#39;AMERICAN KESTREL&#39;, &#39;AMERICAN PIPIT&#39;, &#39;AMERICAN REDSTART&#39;, &#39;ANHINGA&#39;, &#39;ANNAS HUMMINGBIRD&#39;, &#39;ANTBIRD&#39;, &#39;ARARIPE MANAKIN&#39;, &#39;ASIAN CRESTED IBIS&#39;, &#39;BALD EAGLE&#39;, &#39;BALD IBIS&#39;, &#39;BALI STARLING&#39;, &#39;BALTIMORE ORIOLE&#39;, &#39;BANANAQUIT&#39;, &#39;BANDED BROADBILL&#39;, &#39;BANDED PITA&#39;, &#39;BAR-TAILED GODWIT&#39;, &#39;BARN OWL&#39;, &#39;BARN SWALLOW&#39;, &#39;BARRED PUFFBIRD&#39;, &#39;BAY-BREASTED WARBLER&#39;, &#39;BEARDED BARBET&#39;, &#39;BEARDED BELLBIRD&#39;, &#39;BEARDED REEDLING&#39;, &#39;BELTED KINGFISHER&#39;, &#39;BIRD OF PARADISE&#39;, &#39;BLACK &amp; YELLOW bROADBILL&#39;, &#39;BLACK BAZA&#39;, &#39;BLACK FRANCOLIN&#39;, &#39;BLACK SKIMMER&#39;, &#39;BLACK SWAN&#39;, &#39;BLACK TAIL CRAKE&#39;, &#39;BLACK THROATED BUSHTIT&#39;, &#39;BLACK THROATED WARBLER&#39;, &#39;BLACK VULTURE&#39;, &#39;BLACK-CAPPED CHICKADEE&#39;, &#39;BLACK-NECKED GREBE&#39;, &#39;BLACK-THROATED SPARROW&#39;, &#39;BLACKBURNIAM WARBLER&#39;, &#39;BLONDE CRESTED WOODPECKER&#39;, &#39;BLUE COAU&#39;, &#39;BLUE GROUSE&#39;, &#39;BLUE HERON&#39;, &#39;BLUE THROATED TOUCANET&#39;, &#39;BOBOLINK&#39;, &#39;BORNEAN BRISTLEHEAD&#39;, &#39;BORNEAN LEAFBIRD&#39;, &#39;BORNEAN PHEASANT&#39;, &#39;BRANDT CORMARANT&#39;, &#39;BROWN CREPPER&#39;, &#39;BROWN NOODY&#39;, &#39;BROWN THRASHER&#39;, &#39;BULWERS PHEASANT&#39;, &#39;CACTUS WREN&#39;, &#39;CALIFORNIA CONDOR&#39;, &#39;CALIFORNIA GULL&#39;, &#39;CALIFORNIA QUAIL&#39;, &#39;CANARY&#39;, &#39;CAPE GLOSSY STARLING&#39;, &#39;CAPE MAY WARBLER&#39;, &#39;CAPPED HERON&#39;, &#39;CAPUCHINBIRD&#39;, &#39;CARMINE BEE-EATER&#39;, &#39;CASPIAN TERN&#39;, &#39;CASSOWARY&#39;, &#39;CEDAR WAXWING&#39;, &#39;CERULEAN WARBLER&#39;, &#39;CHARA DE COLLAR&#39;, &#39;CHESTNET BELLIED EUPHONIA&#39;, &#39;CHIPPING SPARROW&#39;, &#39;CHUKAR PARTRIDGE&#39;, &#39;CINNAMON TEAL&#39;, &#39;CLARKS NUTCRACKER&#39;, &#39;COCK OF THE ROCK&#39;, &#39;COCKATOO&#39;, &#39;COLLARED ARACARI&#39;, &#39;COMMON FIRECREST&#39;, &#39;COMMON GRACKLE&#39;, &#39;COMMON HOUSE MARTIN&#39;, &#39;COMMON LOON&#39;, &#39;COMMON POORWILL&#39;, &#39;COMMON STARLING&#39;, &#39;COUCHS KINGBIRD&#39;, &#39;CRESTED AUKLET&#39;, &#39;CRESTED CARACARA&#39;, &#39;CRESTED NUTHATCH&#39;, &#39;CRIMSON SUNBIRD&#39;, &#39;CROW&#39;, &#39;CROWNED PIGEON&#39;, &#39;CUBAN TODY&#39;, &#39;CUBAN TROGON&#39;, &#39;CURL CRESTED ARACURI&#39;, &#39;D-ARNAUDS BARBET&#39;, &#39;DARK EYED JUNCO&#39;, &#39;DOUBLE BARRED FINCH&#39;, &#39;DOUBLE BRESTED CORMARANT&#39;, &#39;DOWNY WOODPECKER&#39;, &#39;EASTERN BLUEBIRD&#39;, &#39;EASTERN MEADOWLARK&#39;, &#39;EASTERN ROSELLA&#39;, &#39;EASTERN TOWEE&#39;, &#39;ELEGANT TROGON&#39;, &#39;ELLIOTS PHEASANT&#39;, &#39;EMPEROR PENGUIN&#39;, &#39;EMU&#39;, &#39;ENGGANO MYNA&#39;, &#39;EURASIAN GOLDEN ORIOLE&#39;, &#39;EURASIAN MAGPIE&#39;, &#39;EVENING GROSBEAK&#39;, &#39;FAIRY BLUEBIRD&#39;, &#39;FIRE TAILLED MYZORNIS&#39;, &#39;FLAME TANAGER&#39;, &#39;FLAMINGO&#39;, &#39;FRIGATE&#39;, &#39;GAMBELS QUAIL&#39;, &#39;GANG GANG COCKATOO&#39;, &#39;GILA WOODPECKER&#39;, &#39;GILDED FLICKER&#39;, &#39;GLOSSY IBIS&#39;, &#39;GO AWAY BIRD&#39;, &#39;GOLD WING WARBLER&#39;, &#39;GOLDEN CHEEKED WARBLER&#39;, &#39;GOLDEN CHLOROPHONIA&#39;, &#39;GOLDEN EAGLE&#39;, &#39;GOLDEN PHEASANT&#39;, &#39;GOLDEN PIPIT&#39;, &#39;GOULDIAN FINCH&#39;, &#39;GRAY CATBIRD&#39;, &#39;GRAY KINGBIRD&#39;, &#39;GRAY PARTRIDGE&#39;, &#39;GREAT GRAY OWL&#39;, &#39;GREAT KISKADEE&#39;, &#39;GREAT POTOO&#39;, &#39;GREATOR SAGE GROUSE&#39;, &#39;GREEN BROADBILL&#39;, &#39;GREEN JAY&#39;, &#39;GREEN MAGPIE&#39;, &#39;GREY PLOVER&#39;, &#39;GROVED BILLED ANI&#39;, &#39;GUINEA TURACO&#39;, &#39;GUINEAFOWL&#39;, &#39;GYRFALCON&#39;, &#39;HARLEQUIN DUCK&#39;, &#39;HARPY EAGLE&#39;, &#39;HAWAIIAN GOOSE&#39;, &#39;HELMET VANGA&#39;, &#39;HIMALAYAN MONAL&#39;, &#39;HOATZIN&#39;, &#39;HOODED MERGANSER&#39;, &#39;HOOPOES&#39;, &#39;HORNBILL&#39;, &#39;HORNED GUAN&#39;, &#39;HORNED LARK&#39;, &#39;HORNED SUNGEM&#39;, &#39;HOUSE FINCH&#39;, &#39;HOUSE SPARROW&#39;, &#39;HYACINTH MACAW&#39;, &#39;IMPERIAL SHAQ&#39;, &#39;INCA TERN&#39;, &#39;INDIAN BUSTARD&#39;, &#39;INDIAN PITTA&#39;, &#39;INDIAN ROLLER&#39;, &#39;INDIGO BUNTING&#39;, &#39;IWI&#39;, &#39;JABIRU&#39;, &#39;JAVA SPARROW&#39;, &#39;KAGU&#39;, &#39;KAKAPO&#39;, &#39;KILLDEAR&#39;, &#39;KING VULTURE&#39;, &#39;KIWI&#39;, &#39;KOOKABURRA&#39;, &#39;LARK BUNTING&#39;, &#39;LAZULI BUNTING&#39;, &#39;LILAC ROLLER&#39;, &#39;LONG-EARED OWL&#39;, &#39;MAGPIE GOOSE&#39;, &#39;MALABAR HORNBILL&#39;, &#39;MALACHITE KINGFISHER&#39;, &#39;MALAGASY WHITE EYE&#39;, &#39;MALEO&#39;, &#39;MALLARD DUCK&#39;, &#39;MANDRIN DUCK&#39;, &#39;MANGROVE CUCKOO&#39;, &#39;MARABOU STORK&#39;, &#39;MASKED BOOBY&#39;, &#39;MASKED LAPWING&#39;, &#39;MIKADO PHEASANT&#39;, &#39;MOURNING DOVE&#39;, &#39;MYNA&#39;, &#39;NICOBAR PIGEON&#39;, &#39;NOISY FRIARBIRD&#39;, &#39;NORTHERN CARDINAL&#39;, &#39;NORTHERN FLICKER&#39;, &#39;NORTHERN FULMAR&#39;, &#39;NORTHERN GANNET&#39;, &#39;NORTHERN GOSHAWK&#39;, &#39;NORTHERN JACANA&#39;, &#39;NORTHERN MOCKINGBIRD&#39;, &#39;NORTHERN PARULA&#39;, &#39;NORTHERN RED BISHOP&#39;, &#39;NORTHERN SHOVELER&#39;, &#39;OCELLATED TURKEY&#39;, &#39;OKINAWA RAIL&#39;, &#39;ORANGE BRESTED BUNTING&#39;, &#39;ORIENTAL BAY OWL&#39;, &#39;OSPREY&#39;, &#39;OSTRICH&#39;, &#39;OVENBIRD&#39;, &#39;OYSTER CATCHER&#39;, &#39;PAINTED BUNTIG&#39;, &#39;PALILA&#39;, &#39;PARADISE TANAGER&#39;, &#39;PARAKETT AKULET&#39;, &#39;PARUS MAJOR&#39;, &#39;PATAGONIAN SIERRA FINCH&#39;, &#39;PEACOCK&#39;, &#39;PELICAN&#39;, &#39;PEREGRINE FALCON&#39;, &#39;PHILIPPINE EAGLE&#39;, &#39;PINK ROBIN&#39;, &#39;POMARINE JAEGER&#39;, &#39;PUFFIN&#39;, &#39;PURPLE FINCH&#39;, &#39;PURPLE GALLINULE&#39;, &#39;PURPLE MARTIN&#39;, &#39;PURPLE SWAMPHEN&#39;, &#39;PYGMY KINGFISHER&#39;, &#39;QUETZAL&#39;, &#39;RAINBOW LORIKEET&#39;, &#39;RAZORBILL&#39;, &#39;RED BEARDED BEE EATER&#39;, &#39;RED BELLIED PITTA&#39;, &#39;RED BROWED FINCH&#39;, &#39;RED FACED CORMORANT&#39;, &#39;RED FACED WARBLER&#39;, &#39;RED FODY&#39;, &#39;RED HEADED DUCK&#39;, &#39;RED HEADED WOODPECKER&#39;, &#39;RED HONEY CREEPER&#39;, &#39;RED NAPED TROGON&#39;, &#39;RED TAILED HAWK&#39;, &#39;RED TAILED THRUSH&#39;, &#39;RED WINGED BLACKBIRD&#39;, &#39;RED WISKERED BULBUL&#39;, &#39;REGENT BOWERBIRD&#39;, &#39;RING-NECKED PHEASANT&#39;, &#39;ROADRUNNER&#39;, &#39;ROBIN&#39;, &#39;ROCK DOVE&#39;, &#39;ROSY FACED LOVEBIRD&#39;, &#39;ROUGH LEG BUZZARD&#39;, &#39;ROYAL FLYCATCHER&#39;, &#39;RUBY THROATED HUMMINGBIRD&#39;, &#39;RUDY KINGFISHER&#39;, &#39;RUFOUS KINGFISHER&#39;, &#39;RUFUOS MOTMOT&#39;, &#39;SAMATRAN THRUSH&#39;, &#39;SAND MARTIN&#39;, &#39;SANDHILL CRANE&#39;, &#39;SATYR TRAGOPAN&#39;, &#39;SCARLET CROWNED FRUIT DOVE&#39;, &#39;SCARLET IBIS&#39;, &#39;SCARLET MACAW&#39;, &#39;SCARLET TANAGER&#39;, &#39;SHOEBILL&#39;, &#39;SHORT BILLED DOWITCHER&#39;, &#39;SMITHS LONGSPUR&#39;, &#39;SNOWY EGRET&#39;, &#39;SNOWY OWL&#39;, &#39;SORA&#39;, &#39;SPANGLED COTINGA&#39;, &#39;SPLENDID WREN&#39;, &#39;SPOON BILED SANDPIPER&#39;, &#39;SPOONBILL&#39;, &#39;SPOTTED CATBIRD&#39;, &#39;SRI LANKA BLUE MAGPIE&#39;, &#39;STEAMER DUCK&#39;, &#39;STORK BILLED KINGFISHER&#39;, &#39;STRAWBERRY FINCH&#39;, &#39;STRIPED OWL&#39;, &#39;STRIPPED MANAKIN&#39;, &#39;STRIPPED SWALLOW&#39;, &#39;SUPERB STARLING&#39;, &#39;SWINHOES PHEASANT&#39;, &#39;TAIWAN MAGPIE&#39;, &#39;TAKAHE&#39;, &#39;TASMANIAN HEN&#39;, &#39;TEAL DUCK&#39;, &#39;TIT MOUSE&#39;, &#39;TOUCHAN&#39;, &#39;TOWNSENDS WARBLER&#39;, &#39;TREE SWALLOW&#39;, &#39;TROPICAL KINGBIRD&#39;, &#39;TRUMPTER SWAN&#39;, &#39;TURKEY VULTURE&#39;, &#39;TURQUOISE MOTMOT&#39;, &#39;UMBRELLA BIRD&#39;, &#39;VARIED THRUSH&#39;, &#39;VENEZUELIAN TROUPIAL&#39;, &#39;VERMILION FLYCATHER&#39;, &#39;VICTORIA CROWNED PIGEON&#39;, &#39;VIOLET GREEN SWALLOW&#39;, &#39;VULTURINE GUINEAFOWL&#39;, &#39;WALL CREAPER&#39;, &#39;WATTLED CURASSOW&#39;, &#39;WHIMBREL&#39;, &#39;WHITE BROWED CRAKE&#39;, &#39;WHITE CHEEKED TURACO&#39;, &#39;WHITE NECKED RAVEN&#39;, &#39;WHITE TAILED TROPIC&#39;, &#39;WHITE THROATED BEE EATER&#39;, &#39;WILD TURKEY&#39;, &#39;WILSONS BIRD OF PARADISE&#39;, &#39;WOOD DUCK&#39;, &#39;YELLOW BELLIED FLOWERPECKER&#39;, &#39;YELLOW CACIQUE&#39;, &#39;YELLOW HEADED BLACKBIRD&#39;, &#39;images to test&#39;] . (326, 326) . &lt;&gt; shows the difference between an image that has been zoomed, interpolated, rotated, and then interpolated again (which is the approach used by all other deep learning libraries), shown here on the right, and an image that has been zoomed and rotated as one operation and then interpolated just once on the left (the fastai approach), shown here on the left. . Training: resnet34 . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fine_tune(2) . epoch train_loss valid_loss error_rate time . 0 | 1.281556 | 0.585761 | 0.146585 | 03:51 | . epoch train_loss valid_loss error_rate time . 0 | 0.611539 | 0.314773 | 0.080854 | 03:56 | . 1 | 0.308219 | 0.205119 | 0.050114 | 03:54 | . learn.model . Sequential( (0): Sequential( (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False) (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (2): ReLU(inplace=True) (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False) (4): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (1): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (5): Sequential( (0): BasicBlock( (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (6): Sequential( (0): BasicBlock( (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (3): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (4): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (5): BasicBlock( (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (7): Sequential( (0): BasicBlock( (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (downsample): Sequential( (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False) (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) (1): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) (2): BasicBlock( (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (relu): ReLU(inplace=True) (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False) (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) ) ) ) (1): Sequential( (0): AdaptiveConcatPool2d( (ap): AdaptiveAvgPool2d(output_size=1) (mp): AdaptiveMaxPool2d(output_size=1) ) (1): Flatten(full=False) (2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (3): Dropout(p=0.25, inplace=False) (4): Linear(in_features=1024, out_features=512, bias=False) (5): ReLU(inplace=True) (6): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) (7): Dropout(p=0.5, inplace=False) (8): Linear(in_features=512, out_features=326, bias=False) ) ) . Improving our model . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.lr_find(start_lr=1e-5, end_lr=1e1) . SuggestedLRs(valley=0.003311311127617955) . learn.fine_tune?? . learn = cnn_learner(dls, resnet34, metrics=error_rate) learn.fit_one_cycle(3, 3e-3) . epoch train_loss valid_loss error_rate time . 0 | 1.091636 | 0.601378 | 0.163981 | 03:48 | . 1 | 0.630284 | 0.308223 | 0.079371 | 03:49 | . 2 | 0.422914 | 0.247530 | 0.061184 | 03:47 | . learn.unfreeze() . learn.lr_find() . SuggestedLRs(valley=7.585775892948732e-05) . learn.fit_one_cycle(10, lr_max=9e-5) . epoch train_loss valid_loss error_rate time . 0 | 0.366718 | 0.241547 | 0.060986 | 03:55 | . 1 | 0.392569 | 0.264818 | 0.068202 | 03:57 | . 2 | 0.382351 | 0.240760 | 0.061085 | 03:54 | . 3 | 0.280636 | 0.220931 | 0.055056 | 03:55 | . 4 | 0.223109 | 0.198764 | 0.047346 | 03:54 | . 5 | 0.156686 | 0.174471 | 0.040130 | 03:54 | . 6 | 0.125461 | 0.163392 | 0.039439 | 03:54 | . 7 | 0.095606 | 0.158930 | 0.036276 | 03:54 | . 8 | 0.066164 | 0.154093 | 0.034200 | 03:54 | . 9 | 0.055492 | 0.152054 | 0.034398 | 03:54 | . learn.recorder.plot_loss() .",
            "url": "https://ddemoivre.github.io/blog/jupyter/2022/02/22/_02_09_Image_classification.html",
            "relUrl": "/jupyter/2022/02/22/_02_09_Image_classification.html",
            "date": " • Feb 22, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://ddemoivre.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://ddemoivre.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About",
          "content": "I’m mathematician, I blog about machin learnig and modeling. .",
          "url": "https://ddemoivre.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://ddemoivre.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}